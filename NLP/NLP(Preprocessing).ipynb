{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"/Users/nikita/Documents/Study/Internship/Datasets/aclImdb\"\n",
    "labels ={'pos': 1, 'neg' : 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "data = pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),'r',encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            data = data.append([[txt, labels[l]]],ignore_index=True)\n",
    "            pbar.update()\n",
    "data.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.to_csv('movie.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('movie.csv', encoding='utf-8')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining', \n",
    "                'The weather is sweet',\n",
    "                'The sun is shuning, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2',smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[10,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '',text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-',''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor(data.loc[10,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.loc[:25000,'review'].values\n",
    "y_train = data.loc[:25000, 'sentiment'].values\n",
    "X_test = data.loc[25000:, 'review'].values\n",
    "y_test = data.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 163.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acc...\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7f903508ab90>,\n",
       "                                              <function tokenizer_porter at 0x7f903508ad40>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range':[(1,1)],\n",
    "              'vect__stop_words':[stop,None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C': [1.0,10.0,100.0]},\n",
    "             {'vect__ngram_range':[(1,1)],\n",
    "              'vect__stop_words':[stop,None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'vect__use_idf': [False],\n",
    "              'vect__norm': [None],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C': [1.0,10.0,100.0]}]\n",
    "lr_tfidf = Pipeline([('vect',tfidf),('clf', LogisticRegression(random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy',cv=5,verbose=1,n_jobs=1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__norm', 'vect__preprocessor', 'vect__smooth_idf', 'vect__stop_words', 'vect__strip_accents', 'vect__sublinear_tf', 'vect__token_pattern', 'vect__tokenizer', 'vect__use_idf', 'vect__vocabulary', 'clf__C', 'clf__class_weight', 'clf__dual', 'clf__fit_intercept', 'clf__intercept_scaling', 'clf__l1_ratio', 'clf__max_iter', 'clf__multi_class', 'clf__n_jobs', 'clf__penalty', 'clf__random_state', 'clf__solver', 'clf__tol', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший набор параметров: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7f903508ab90>} \n"
     ]
    }
   ],
   "source": [
    "print('Наилучший набор параметров: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность при перекрестной проверке: 0.893\n"
     ]
    }
   ],
   "source": [
    "print('Правильность при перекрестной проверке: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность при испытании: 0.900\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Правильность при испытании: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
